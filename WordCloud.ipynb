{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import tweepy\n",
    "import json\n",
    "import MeCab\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import codecs\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "access_token = \n",
    "access_token_secret = \n",
    "consumer_key = \n",
    "consumer_key_secret = \n",
    "\n",
    "#twitterAPIのためのconfigファイル\n",
    "#JSON_LOAD_FILE = r\"./json/twitter.json\"\n",
    "#twitterからデータを抽出するときの検索ワード\n",
    "search_word = \"野菜\"\n",
    "#twitterからデータを抽出するときの取得件数\n",
    "SEARCH_COUNT = 100\n",
    "\n",
    "#tweeterAPIと繋げるAPIインスタンスを生成する\n",
    "def twitter_api_connect():\n",
    "    #twiteerのAPIを利用するためのキーを取得\n",
    "\n",
    "\n",
    "    #twiteerのAPIを利用するための、認証データ作成\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "    auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "    #twitterのAPIインスタンス生成\n",
    "    twitter_api = tweepy.API(auth, wait_on_rate_limit = True)\n",
    "    return twitter_api\n",
    "\n",
    "#tweetのデータを取得し、リスト化し、返却する\n",
    "def tweet_list_create(twitter_api, search_word, search_count):\n",
    "    #カーソルを使用してデータ取得\n",
    "    search_tweets = twitter_api.search_users(q = search_word, count = search_count,lang=\"ja\")\n",
    "\n",
    "    #tweetの内容を格納するためのリスト変数\n",
    "    search_tweet_list = []\n",
    "\n",
    "    #取得したtweetの内容をリストに格納\n",
    "    for search_tweet in search_tweets:\n",
    "        #print(search_tweets)\n",
    "        search_tweet_list.append(search_tweet.description) \n",
    "    return search_tweet_list\n",
    "\n",
    "\n",
    "\n",
    "#twitterのAPIインスタンス生成\n",
    "api = twitter_api_connect()\n",
    "\n",
    "#twitterからデータを取得\n",
    "tweet_list = tweet_list_create(api, search_word , SEARCH_COUNT)\n",
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#アカウント指定(@の後ろの英数字、↓はサンプルID)\n",
    "Account = ''\n",
    "\n",
    "#ツイート保存用変数&取得するツイートを計算する\n",
    "tweet_data = []\n",
    "num = 0\n",
    "\n",
    "for page in range(17):\n",
    "    tweets = api.user_timeline(screen_name=Account, count=200, page=page)\n",
    "    for tweet in tweets:\n",
    "        tweet.created_at += timedelta(hours=9)\n",
    "#         print('----------') #見やすいように\n",
    "#         print(tweet.created_at) #書くツイートの投稿時間表示\n",
    "#         print(tweet.text) #各ツイート内容表示\n",
    "        num += 1\n",
    "        tweet_data.append(['@'+Account, tweet.created_at, tweet.text])\n",
    "\n",
    "print(num, 'ツイート表示しました。')\n",
    "\n",
    "# ツイートデータをデータフレームに変換\n",
    "tweet_data = pd.DataFrame(tweet_data, columns=[\"account\", \"date\", \"tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 極性辞書をデータフレームで読み込み\n",
    "df_dic = pd.read_csv('pn_ja.dic', sep=':', names=(\"Word\", \"読み\", \"品詞\", \"Score\"), encoding='shift-jis')\n",
    "\n",
    "keys = df_dic[\"Word\"].tolist()\n",
    "values = df_dic[\"Score\"].tolist()\n",
    "dic = dict(zip(keys, values))\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\"\n",
    "r = requests.get(url)\n",
    "tmp = r.text.split('\\r\\n')\n",
    "stopwords = []\n",
    "for i in range(len(tmp)):\n",
    "    if len(tmp[i]) < 1:\n",
    "        continue\n",
    "    stopwords.append(tmp[i])\n",
    "print(stopwords)\n",
    "\n",
    "#####joblibで保存して使いたい時     \n",
    "import joblib\n",
    "joblib.dump(stopwords, 'stopwords.jb', compress = 3)\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "stopwords = joblib.load('stopwords.jb')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "\n",
    "t = Tokenizer()\n",
    "tweet_list = []\n",
    "\n",
    "for tweet in tweet_data['tweet']:\n",
    "    # 複合名詞を抽出\n",
    "    a = Analyzer(token_filters=[CompoundNounFilter()])\n",
    "    #result = [token.base_form for token in a.analyze(tweet) if token.part_of_speech.split(',')[0] in ['名詞']]\n",
    "    # 動詞と形容詞を抽出\n",
    "    result = [token.base_form for token in t.tokenize(tweet)\n",
    "       if token.part_of_speech.split(',')[0] in ['動詞']]\n",
    "    \n",
    "    result.extend(result2)\n",
    "    result_list = [tweet, result]\n",
    "    tweet_list.append(result_list)\n",
    "tweets = tweet_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweet_list:\n",
    "    for stopword in stopwords:\n",
    "        tweet[1] = [i for i in tweet[1] if i != stopword]\n",
    "    tweet_word_num = len(tweet[1])\n",
    "    tweet.append(tweet_word_num)\n",
    "    # 辞書で判定\n",
    "    results = []\n",
    "    for sentence in tweet[1]:\n",
    "        word_score = []\n",
    "        score = dic.get(sentence)\n",
    "        word_score = (sentence, score)\n",
    "        results.append(word_score)\n",
    "\n",
    "    # 判定を追加\n",
    "    tweet.append(results) \n",
    "    # 曲のスコアを2パターンで判定。「Noneを含むパターン」と「Noneを含まないパターン」\n",
    "    score = 0\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for word in tweet[3]:\n",
    "        if word[1] != None:\n",
    "            score += word[1]\n",
    "    cnt = sum(not (i[1] == None) for i in tweet[3])  # Noneを含まない形態素数\n",
    "    if score != 0:\n",
    "        score1 = score / tweet[2]  # スコアの合計 / 全形態素数\n",
    "        score2 = score / cnt  # スコアの合計 / Noneを含まない形態素数\n",
    "    # スコアを追加\n",
    "    tweet.append(cnt)\n",
    "    tweet.append(score1)\n",
    "    tweet.append(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tubu = []\n",
    "score_list = []\n",
    "score_list2 = []\n",
    "\n",
    "for i in range(len(tweet_list)):\n",
    "    tubu.append(tweet_list[i][0])\n",
    "    score_list.append(tweet_list[i][5])\n",
    "    score_list2.append(tweet_list[i][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#辞書に無い単語は無かったものとしてスコアを算出する（スコア1）\n",
    "#辞書に無い単語はスコア 0 として算出する（スコア2）\n",
    "\n",
    "np_df = pd.DataFrame(tubu)\n",
    "np_df[1] = score_list\n",
    "np_df[2] = score_list2\n",
    "np_df.columns = ['text','score','score2']\n",
    "np_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_df = np_df[np_df['score'] != 0].reset_index(drop=True)\n",
    "np_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_df['score'].mean(),np_df['score2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i in range(len(np_df)):\n",
    "    word_list.extend(tweet_list[i][1])\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [re.sub('@.*', '', s) for s in word_list]\n",
    "word_list = [re.sub('https.*', '', s) for s in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Counter(word_list)\n",
    "a.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_list = ['する','ん','てる','の','なる','いる','ある','言う','ない','いい','られる','しまう','','ー','w','(',')','`)','☺']\n",
    "\n",
    "word_stoped_list =[w for w in word_list if w not in s_list]\n",
    "word_stoped_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#wordcloud用のフォントパス\n",
    "WORDCLOUD_FONT_PATH = 'C:\\Windows\\Fonts\\yumin.ttf'\n",
    "\n",
    "#Mac\n",
    "#'/System/Library/Fonts/ヒラギノ明朝 ProN.ttc'\n",
    "\n",
    "#Windows版\n",
    "#'C:\\Windows\\Fonts\\yumin.ttf'\n",
    "\n",
    "#wordcloud用の幅\n",
    "WORDCLOUD_WIDTH = 800\n",
    "#wordcloud用の高さ\n",
    "WORDCLOUD_HEIGHT = 500\n",
    "#wordcloud用の背景色\n",
    "WORDCLOUD_BG_COLOR = 'white'\n",
    "#twitterからデータを抽出するときの検索ワード\n",
    "SEARCH_WORD = \"negapoji_動詞cloud\"\n",
    "\n",
    "def word_cloud_png_creater(word_list, font_path, width, height, bg_color, search_word):\n",
    "    word_counter = Counter(word_list)\n",
    "    wc = WordCloud(font_path=font_path,\n",
    "                   width=width, \n",
    "                   height=height, \n",
    "                   background_color=bg_color).generate_from_frequencies(word_counter)\n",
    "    wc.to_file('./photo/{}.png'.format(search_word))\n",
    "\n",
    "#フォルダに、検索文字列.pngでwordcloud作成\n",
    "word_cloud_png_creater(word_stoped_list, WORDCLOUD_FONT_PATH, WORDCLOUD_WIDTH, WORDCLOUD_HEIGHT, WORDCLOUD_BG_COLOR, SEARCH_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('./data/ネガポジUnknown.csv',encoding = 'ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(file['score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考\n",
    "https://zatsugaku-engineer.com/python/negaposi/\n",
    "https://qiita.com/ChabesuB/items/e451f9a3882433658b27"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
