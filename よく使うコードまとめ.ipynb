{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ほぼ絶対使用\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import itertools\n",
    "from hyperopt import fmin,tpe,hp,STATUS_OK,Trials\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,auc,log_loss,roc_curve,roc_auc_score\n",
    "\n",
    "#場合により使用\n",
    "from sklearn.svm import LinearSVC\n",
    "import cv2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,auc,log_loss,roc_curve,roc_auc_score\n",
    "import csv\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ファイルのインポートと説明変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('..//Downloads//1.train_data.csv'encoding=ANSI)\n",
    "\n",
    "tr_x=train2.drop( 'カラム名',axis=1)\n",
    "tr_y=train2['応答変数']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数値データとカテゴリデータをわける\n",
    "num_features = [数値データ]\n",
    "\n",
    "cat_features = [カテゴリデータ]\n",
    "\n",
    "features = num_features + cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリ変数を数値に変換\n",
    "le = LabelEncoder()\n",
    "le_dict = {}\n",
    "for col in cat_features:\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "    le_dict['col']=le\n",
    "\n",
    "#タイプをcategory型に変換する\n",
    "train[cat_features] = train[cat_features].astype('int')\n",
    "test[cat_features] = test[cat_features].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 欠損値の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本情報の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相互特徴量の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in num_features:\n",
    "    for v in num_features:\n",
    "        if c==v:\n",
    "            continue\n",
    "        combine[c+\"+\"+v]=combine[c]+combine[v]\n",
    "        combine[c+\"-\"+v]=combine[c]-combine[v]\n",
    "        combine[c+\"-\"+v]=combine[c]-combine[v]\n",
    "        combine[c+\"-\"+v]=abs(combine[c+\"-\"+v])\n",
    "        if 0 in combine[c] or 0 in combine[v]== True:\n",
    "            combine[c+\"*\"+v]=0\n",
    "        combine[c+\"/\"+v]=combine[c]/combine[v]\n",
    "        if 0 in combine[c] or 0 in combine[v]== True:\n",
    "            combine[c+\"/\"+v]=0\n",
    "            \n",
    "for c in combine.columns:\n",
    "    con = collections.Counter(combine[c]==0)\n",
    "    if con[1] == combine.shape[0]:\n",
    "        combine=combine.drop(c,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeyword(text):  \n",
    "      #\\\\\\textを形態素解析して、名詞のみのリストを返す\\\\\\  \n",
    "    tagger = MeCab.Tagger()  \n",
    "    encoded_text=text  \n",
    "    #encoded_text = text.encode('utf-8')  \n",
    "    #print(encoded_text)  \n",
    "    #encoded_text = encoded_text.replace('\\ '  ' ')  \n",
    "    node = tagger.parseToNode(encoded_text).next  \n",
    "    keywords = []\n",
    "    while node:  \n",
    "        if node.feature.split(' ')[0] == \"名詞\":  \n",
    "            keywords.append(node.surface)  \n",
    "        node = node.next  \n",
    "    return keywords  \n",
    "\n",
    "def splitDocument(documents):  \n",
    "  \n",
    "      #\\\\\\文章集合を受け取り、名詞のみ空白区切りの文章にして返す\\\\\\  \n",
    "    splitted_documents = []  \n",
    "  \n",
    "    for d in documents:  \n",
    "        keywords = extractKeyword(d)  \n",
    "        splitted_documents.append(' '.join(keywords))  \n",
    "    return splitted_documents  \n",
    "      \n",
    "      \n",
    "def removeStoplist(documents,stoplist):  \n",
    "      #\\\\\\ストップワードを取り除く\\\\\\\\  \n",
    "    stoplist_removed_documents = []  \n",
    "    for document in documents:  \n",
    "        words = []  \n",
    "        if isinstance(document,int) == True:  \n",
    "            document=str(document)  \n",
    "        for word in document.lower().split():  \n",
    "            if word not in stoplist:  \n",
    "                words.append(word)  \n",
    "        stoplist_removed_documents.append(words)  \n",
    "    return stoplist_removed_documents  \n",
    "      \n",
    "      \n",
    "def getTokensOnce(all_tokens):  \n",
    "  \n",
    "      \n",
    "    #  u\\\\\\一回しか出現しない単語を返す\\\\\\  \n",
    "    tokens_once = set([])  \n",
    "      \n",
    "    for word in set(all_tokens):  \n",
    "        if all_tokens.count(word) == 1:  \n",
    "            tokens_once.add(word)  \n",
    "    return tokens_once  \n",
    "       \n",
    "def removeTokensOnce(documents,tokens_once):  \n",
    "    #  u\\\\\\一回しか出現しない単語を取り除く\\\\\\  \n",
    "    token_removed_documents = []  \n",
    "    for document in documents:  \n",
    "        words = []  \n",
    "        for word in document:  \n",
    "            if word not in tokens_once:  \n",
    "                words.append(word)  \n",
    "        token_removed_documents.append(words)  \n",
    "    return token_removed_documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \\__main__\\:  \n",
    "    raw_documents =df_train['text']  \n",
    "  \n",
    "      # 空白区切りの文字列を入れる  \n",
    "    splitted_documents = splitDocument(raw_documents)  \n",
    "      \n",
    "    print(\\splitted_documents:\\)  \n",
    "    #for d in splitted_documents:  \n",
    "        #print(d)  \n",
    "        #print(\\\\)  \n",
    "      \n",
    "    stoplist = set([  \n",
    "    #    'の'  'に'  'て'  '、'  'が'  'た'  '。'  'は'  'を'  'で' '.' '\\ ' '/' 't' 'https' '://' 'co'  \n",
    "    ]) # 通常の文字列でよい  \n",
    "    print(\\stoplist:\\)  \n",
    "    #for s in stoplist:  \n",
    "        #print(s)  \n",
    "      \n",
    "      # ストップワードを除いた二重リスト  \n",
    "    stoplist_removed_documents = removeStoplist(splitted_documents  stoplist)  \n",
    "      \n",
    "    print(\\stoplist_removed_documents:\\)  \n",
    "    #for t in stoplist_removed_documents:  \n",
    "        #for w in t:  \n",
    "            #print(w)   \n",
    "            #print(\\\\)  \n",
    "        #print(\\\\)  \n",
    "      \n",
    "      # 全ての単語を重複ありで含むリスト  \n",
    "    all_tokens = ' '.join(stoplist_removed_documents)  \n",
    "    print(\\all_tokens:\\)  \n",
    "    #for t in all_tokens:  \n",
    "        #print(t)   \n",
    "        #print(\\\\)  \n",
    "      \n",
    "      # 一回しかでてこない単語のみを持つセット  \n",
    "    tokens_once = getTokensOnce(all_tokens)  \n",
    "    print(\\tokens_once:\\)  \n",
    "    #for t in tokens_once:  \n",
    "        #print(t)   \n",
    "        #print(\\\\)  \n",
    "      # 一回しかでてこない単語を除いた最終的なテキスト  \n",
    "    preprocessed_documents = removeTokensOnce(stoplist_removed_documents  tokens_once)  \n",
    "    print(\\preprocessed_documents:\\)  \n",
    "    #for d in preprocessed_documents:  \n",
    "        #for w in d:  \n",
    "            #print(w)   \n",
    "        #print(\\\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この関数を使って，textを分かち書き．\n",
    "# applyをすることでtextの1行1行に対して分かち書き．\n",
    "\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "df_train['text'] = df_train['text'].apply(mecab.parse)\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "# targetが「0」つまり，災害に関係ないツイートの上位20件の頻出単語です\n",
    "df_train.groupby('flg')['text'].apply(lambda x: Counter(' '.join(x).split(' ')).most_common(n=20)).iloc[0]\n",
    "\n",
    "\n",
    "# targetが「0」の場合の上位10件の頻出単語をストップワードとして指定\n",
    "stop_words = set([\n",
    "    'の', 'に', 'て', '、', 'が', 'た', '。', 'は', 'を', 'で'\n",
    "])\n",
    "df_train.groupby('flg')['text'].apply(lambda x: Counter([w for w in ' '.join(x).split(' ') if w not in stop_words]).most_common(n=20)).iloc[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストデータの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング用データをトレーニング用/バリデーション用に分割，バリデーションは2割分\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# indexをリセット\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クロスバリデーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "scores=[]\n",
    "\n",
    "# KFoldクラスを用いてクロスバリデーションの分割を行う\n",
    "kf=KFold(n_splits=4,shuffle=True,random_state=71)\n",
    "for tri_x,vai_x in kf.split(train_x):\n",
    "    tr_x,va_x = train_x.iloc[tri_x],train_x.iloc[vai_x]\n",
    "    tr_y,va_y = train_y.iloc[tri_x],train_y.iloc[vai_x]\n",
    "    \n",
    "    #モデルを用いて予測（サンプル）\n",
    "    model=Model()\n",
    "    model.fit(tr_x,tr_y,va_x,va_y)\n",
    "    va_pred=model.predict(va_x)\n",
    "    score=log_loss(va_y,va_pred)\n",
    "    scores.appemd(score)\n",
    "    \n",
    "#各スコアの平均をとる\n",
    "print(np.mean(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beg Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizerを定義します\n",
    "cv = CountVectorizer()\n",
    "tf = TfIdfVectorizer()\n",
    "\n",
    "# CountVectorizerに訓練用のtextを学習させます\n",
    "cv.fit(train_df['text'].values)\n",
    "cv.get_feature_names()[:10]\n",
    "\n",
    "# トレーニング用データにCountVectorizerを適用\n",
    "X_train = cv.transform(train_df['text'].values)\n",
    "y_train = train_df['flg'].values\n",
    "\n",
    "# バリデーション用データにCountVectorizerを適用\n",
    "X_val = cv.transform(val_df['text'].values)\n",
    "y_val = val_df['flg'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルいろいろ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ハイパーパラメータ調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル作成\n",
    "params=lgb_params()\n",
    "#探索回数\n",
    "max_evals=10\n",
    "trials=Trials()\n",
    "history=[]\n",
    "#パラメータを変える\n",
    "fmin(lgbscore,params,algo=tpe.suggest,trials=trials,max_evals=max_evals)\n",
    "\n",
    "history=sorted(history,key=lambda tpl:tpl[1])\n",
    "best=history[0]\n",
    "print(f'best params:{best[0]},score:{best[1]:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作り方\n",
    "\n",
    "params→model→scoreの順"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル作成\n",
    "params=lgb_params()\n",
    "max_evals=10\n",
    "trials=Trials()\n",
    "history=[]\n",
    "fmin(lgbscore,params,algo=tpe.suggest,trials=trials,max_evals=max_evals)\n",
    "\n",
    "history=sorted(history,key=lambda tpl:tpl[1])\n",
    "best=history[0]\n",
    "print(f'best params:{best[0]},score:{best[1]:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サポートベクタマシン（分類）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVCを定義します(random_state=0でseed値を固定)\n",
    "l_svc = LinearSVC(random_state=0)\n",
    "\n",
    "# fitで学習します\n",
    "l_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サポートベクタマシン（回帰）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opencv p196\n",
    "def train_svm(tr_x,tr_y):\n",
    "    svm=cv2.ml.SVM_create()\n",
    "    svm.train(tr_x,cv2.ml.ROW_SAMPLE,y_train)\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_Model(params):\n",
    "    params['max_depth']=int(params['max_depth'])\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_val, label=y_val)\n",
    "    num_round=10000\n",
    "    #watchlist=[(dtrain,'train'),(dtest,'eval')]\n",
    "    model=xgb.train(params,dtrain,num_round)\n",
    "    va_pred=model.predict(dtest)\n",
    "    va_pred=np.where(va_pred>0.5,1,0)\n",
    "    return va_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgbscore(params):\n",
    "    xgb_model=xgb_Model(params)\n",
    "    va_pred=xgb_model.predict(X_val2)\n",
    "    va_pred=np.where(va_pred>0.5,1,0)\n",
    "    score=f1_score(y_val,va_pred)\n",
    "    history.append((params,score))\n",
    "    return {'loss': score,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_params():\n",
    "    params={\n",
    "        'booster':'gbtree',\n",
    "        'objective':'binary:logistic',\n",
    "        'eta':0.1,\n",
    "        'gamma':hp.loguniform('gamma',np.log(1e-8),np.log(1.0)),\n",
    "        'max_depth':hp.quniform('max_depth',3,9,1),\n",
    "        'min_child_weight':hp.loguniform('min_child_weight',np.log(0.1),np.log(10)),\n",
    "        'colsample_bytree':hp.quniform('colsample',0.6,0.95,0.05),\n",
    "        'subsample':hp.quniform('subsample',0.6,0.95,0.05),\n",
    "        'eval_metric': 'error',\n",
    "        'early_stopping_rounds': 100,\n",
    "        'random_state':71,\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基準のparam\n",
    "params={\n",
    "        'objective':'binary',\n",
    "        'metric':'binary_error',\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth':5,\n",
    "        'min_data_in_leaf':5,\n",
    "        'seed':71,\n",
    "        'feature_fraction':0.8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_params():\n",
    "    params={\n",
    "        'objective':'binary',\n",
    "        'metric':'binary_error',\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth':hp.quniform('max_depth',3,9,5),\n",
    "        'min_data_in_leaf':hp.quniform('min_data_in_leaf',3,5,9),\n",
    "        'seed':71,\n",
    "        'feature_fraction':0.8,\n",
    "        'bagging_fraction':hp.quniform('bagging_fraction',0.6,0.8,0.2),\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbscore(params):\n",
    "    lgb_model=lgb_Model(params)\n",
    "    va_pred=lgb_model.predict(X_val)\n",
    "    va_pred=np.where(va_pred>0.5,1,0)\n",
    "    score=f1_score(y_val,va_pred)\n",
    "    history.append((params,score))\n",
    "    return {'loss': score,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_Model(params):\n",
    "    params['max_depth']=int(params['max_depth'])\n",
    "    params['min_data_in_leaf']=int(params['min_data_in_leaf'])\n",
    "    lgb_train=lgb.Dataset(X_train,y_train)\n",
    "    lgb_eval=lgb.Dataset(X_val,y_val)\n",
    "    #categorical_features = ['predict']\n",
    "    \n",
    "    lgbmodel = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=(lgb_train, lgb_eval),\n",
    "                num_boost_round=10000,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=50)\n",
    "    return lgbmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#回帰の場合はRidge mse\n",
    "\n",
    "scaler=StandardScaler()\n",
    "tr_x=scaler.fit_transform(tr_x)\n",
    "va_x=scaler.fit_transform(va_x)\n",
    "test_x=scaler.fit_transform(test_x)\n",
    "\n",
    "model=LogisticRegression(C=1.0)\n",
    "model.fit(tr_x,tr_y)\n",
    "\n",
    "#バリデーションデータでのスコアの確認\n",
    "#確率を出力\n",
    "va_pred=model.predict_proba(va_x)\n",
    "score=log_loss(va_y,va_pred)\n",
    "print(f'log_loss:{score:4f}')\n",
    "\n",
    "#予測\n",
    "pred=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing impor StandardScaler\n",
    "\n",
    "#データのスケーリング\n",
    "scaler=StandardScaler()\n",
    "tr_x=scaler.fit_transform(tr_x)\n",
    "va_x=scaler.fit_transform(va_x)\n",
    "test_x=scaler.fit_transform(test_x)\n",
    "#モデルの構築\n",
    "model=Sequential()\n",
    "model.add(Dense(256,activation='relu',input_shape=(train_x.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "#学習の実行\n",
    "#バリデーションデータもモデルに渡し、学習の振興とともにスコアがどう変わるかモニタリングする\n",
    "batch_size=120\n",
    "epochs=10\n",
    "history=model.fit(tr_x,tr_y,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(va_x,va_y))\n",
    "\n",
    "#スコアの確認\n",
    "va_pred=model.predict(va_x)\n",
    "score=log_loss(va_y,va_pred,eps=1e-7)\n",
    "print(f'log_loss:{score:4f}')\n",
    "\n",
    "#予測\n",
    "pred=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コールバック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "学習時、ミニバッチの処理やエポックごとに指定した処理を走らせることができる\n",
    "\n",
    "アーリーストッピング\n",
    "モデルの保存\n",
    "学習のスケジューリング\n",
    "ログ、可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "#アーリーストッピング(20)\n",
    "epochs=50\n",
    "early_stopping=EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True)\n",
    "\n",
    "history=model.fit(tr_x,tr_y,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(va_x,va_y),callbacks=[early_stopping])\n",
    "pred=model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import ReLU,PReLU\n",
    "from keras.layers.core import Dense,Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD,Adam\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params):\n",
    "    model-MLP(params)\n",
    "    model.fit(tr_x,tr_y,va_x,va_y)\n",
    "    va_pred=model.predict(va_x)\n",
    "    score=log_loss(va_y,va_pred)\n",
    "    print(f'log_loss:{score:4f}')\n",
    "    \n",
    "    #情報を記録しておく\n",
    "    history.append((params,score))\n",
    "    \n",
    "    return {'loss':score,'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_param={\n",
    "    'input_dropout':0.0,\n",
    "    'hidden_layers':3,\n",
    "    'hidden_units':96,\n",
    "    'hidden_acivation':'relu',\n",
    "    'hidden_dropout':0.2,\n",
    "    'batch_norm':'before_act',\n",
    "    'optimizer':{'tyoe':'adam','lr':0.001},\n",
    "    'beach_size':64\n",
    "}\n",
    "\n",
    "param_space={\n",
    "    'input_dropout':hp.quniform('input_dropout',0,0.2,0.05),\n",
    "    'hidden_layers':hp.quniform('hidden_layers',2,4,1),\n",
    "    'hidden_units':hp.quniform('hidden_units',32,256,32),\n",
    "    'hidden_acivation':hp.choice('hidden_activation',['prelu','relu']),\n",
    "    'hidden_dropout':hp.quniform('hidden_dropout',0,0.3,0.05),\n",
    "    'batch_norm':hp.choice('batch_norm',['before_act','no']),\n",
    "    'optimizer':hp.choice('optimizer',[{'type':'adam','lr';hp.loguniform('adam_lr',np.log(0.00001),np.log(0.01))}]),\n",
    "    'beach_size':hp.quniform('batch_size',32,128,32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,params):\n",
    "        self.params=params\n",
    "        self.scaler=None\n",
    "        self.model=None\n",
    "        \n",
    "    def fit(self,tr_x,tr_y,va_x,va_y):\n",
    "        input_dropout=self.params['input_dropout']\n",
    "        hidden_layers=int(self.params['hidden_layers'])\n",
    "        hidden_units=int(self.params['hidden_units'])\n",
    "        hidden_activation=self.params['hidden_activation']\n",
    "        hidden_dropout=self.params['hidden_dropout']\n",
    "        betch_norm=self.params['beach_norm']\n",
    "        optimizer_tyoe=self.params['optimizer']['tyoe']\n",
    "        optimizer_lr=self.params['optimizer']['lr']\n",
    "        batch_size=int(self.params['batch_size'])\n",
    "        \n",
    "        #標準化\n",
    "        self.scaler=StandardScaler()\n",
    "        tr_x=self.scaler.fit_transform(tr_x)\n",
    "        va_x=self.scaler.fit_transform(va_x)\n",
    "        \n",
    "        self.model=Sequential()\n",
    "        \n",
    "        #入力層\n",
    "        self.model.add(Dropout(input_dropout,input_shape=(tr_x.shape[1],)))\n",
    "        \n",
    "        #中間層\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm=='before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation=='prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation='relu':\n",
    "                self.model.add(Relu())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "        \n",
    "        #出力層\n",
    "        self.model.add(Dencse(1,activation='sigmoid'))\n",
    "        \n",
    "        #オプティマイザ\n",
    "        if optimizer_type='sgd':\n",
    "            optimizer=SGD(lr=optimizer_lr,decay=1e-6,momentum=0.9,nesterov=True)\n",
    "        elif optimizer_type='adam':\n",
    "            optimizer=ADAM(lr=optimizer_lr,beta_1=0.9,beta_2=0.999,decay=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        #目的変数、評価指標などの設定\n",
    "        self.model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "        \n",
    "        epochs=200\n",
    "        patience=20\n",
    "        early_stopping=EarlyStopping(patience=patience,restore_best_weights=True)\n",
    "        \n",
    "        #学習の実行\n",
    "        \n",
    "        history=self.model.fit(tr_x,tr_y,epochs=epochs,batch_size=batch_size,verbose=1,validation_data=(va_x,va_y),callbacks=early_stopping)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,x):\n",
    "        x=self.scaler.transfform(x)\n",
    "        y_pred=self.model.predict(x)\n",
    "        y_pred=y_pred.flatten()\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 精度評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 混同行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized confusion matrix')\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_val, l_svc.predict(X_val))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "plot_confusion_matrix(cnf_matrix, classes=[i for i in range(2)],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出手順"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用のコード　パラメータはモデルに合わせて設定する\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "df_test['text'] = df_test['text'].apply(mecab.parse)\n",
    "X_test = cv.transform(df_test['text'].values)\n",
    "X_test = X_test.astype(float)\n",
    "params={\n",
    "        'objective':'binary',\n",
    "        'metric':'binary_error',\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth':5,\n",
    "        'min_data_in_leaf':5,\n",
    "        'seed':71,\n",
    "        'feature_fraction':0.8,\n",
    "}\n",
    "model=lgb_Model(params)\n",
    "\n",
    "\n",
    "# 予測しています\n",
    "predict =model.predict(X_test)\n",
    "predict=np.where(predict>0.5,1,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラムのみのcsvファイルsubmission.csvを用意する\n",
    "submission = pd.read_csv('./data/3.submission.csv')\n",
    "\n",
    "#予想結果をカラムへ代入\n",
    "submission['flg'] = predict\n",
    "\n",
    "#ファイルへ書き出し、保存\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相関係数\n",
    "相関係数が大きければその特徴量を選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs=[]\n",
    "for c in tr_x.columns:\n",
    "    corr=np.corrcoef(tr_x[c],tr_y)[0,1]\n",
    "    corrs.append(corr)\n",
    "corrs=np.array(corrs)\n",
    "\n",
    "corrsp=[]\n",
    "for c in tr_x.columns:\n",
    "    corrp=st.spearmanr(train4[c],tr_y).correlation\n",
    "    corrsp.append(corrp)\n",
    "corrsp=np.array(corrsp)\n",
    "    \n",
    "    \n",
    "idx=np.argsort(np.abs(corrs))[::-1]\n",
    "topcols,topimp=train4.columns.values[idx][:5],corrs[idx][:5]\n",
    "print(topcols,topimp)\n",
    "\n",
    "    \n",
    "idx2=np.argsort(np.abs(corrsp))[::-1]\n",
    "topcols2,topimp2=train4.columns.values[idx2][:5],corrs[idx2][:5]\n",
    "print(topcols2,topimp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カイ二乗統計量による特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler as MM\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "#カイ二条統計量\n",
    "x=MM().fit_trainsform(train_x)\n",
    "c2,_=chi2(x,train_y)\n",
    "\n",
    "#重要度の上位を出力\n",
    "\n",
    "idx=np.argsort(c2)[::-1]\n",
    "topcols,topimp=train_x.colmuns.values[idx][:5],corrs[idx][:5]\n",
    "print(topcols,topimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相互情報量による特徴量選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif as mc\n",
    "\n",
    "#相互特徴量\n",
    "mi=mc(train_x,train_y)\n",
    "\n",
    "#重要度の順位を出力する\n",
    "\n",
    "idx=np.argsort(c2)[::-1]\n",
    "topcols,topimp=train_x.colmuns.values[idx][:5],corrs[idx][:5]\n",
    "print(topcols,topimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# threshold_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
